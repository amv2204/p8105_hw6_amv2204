---
title: "Homework 6"
author: "Ashwini Varghese"
date: "2022-12-03"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
library(mgcv)
library(modelr)
library(viridis)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  fig.width = 8,
  fig.asp = 1,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1


```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```


```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```


```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```


## Problem 2

We will start by loading the homicide data. 

```{r}
homicides = read.csv("./homicide-data.csv")
```

Next we will do some data cleaning as below:

```{r}
homicides =   
  homicides %>%
  unite("city_state", city:state, sep = ", ", remove = FALSE) %>% 
  mutate(solved = as.numeric(disposition == "Closed by arrest")) %>%
  filter(!(city %in% c("Dallas", "Phoenix", "Kansas City"))) %>%
  filter(city_state != "Tulsa, AL") %>%
  mutate(victim_age = as.numeric(victim_age)) %>%
  filter(victim_race %in% c("White", "Black")) %>%
  mutate(victim_race = fct_relevel(victim_race, "White")) %>%
  filter(victim_sex != "Unknown")

```

Now for just the city of Baltimore, we will fit a logistic model with homicide being solved as the outcome and with victim age, sex, and race as predictors. 

```{r}

Balt =
  homicides %>%
  filter(city == "Baltimore") %>%
  select(solved, victim_age, victim_race, victim_sex)

fit_balt = 
  Balt %>% 
  glm(solved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) 

fit_balt %>% 
  broom::tidy() %>%
  mutate(lower_CI = exp(estimate - 1.96*std.error),
         upper_CI = exp(estimate + 1.96*std.error),
         OR = exp(estimate)) %>%
  select(term, OR, lower_CI, upper_CI) %>% 
  knitr::kable(digits = 3)


```

Let's repeat this for all the cities and get the OR with their 95% CI for solving homicides comparing males to females, adjusting for race and age.  

```{r}
homi_nest =
  homicides %>% 
  select(city_state, solved, victim_age, victim_race, victim_sex) %>% 
  relocate(city_state) %>% 
  nest(data = solved:victim_sex)


fit_all = 
  homi_nest %>% 
  mutate(
    models = map(.x = data, ~glm(solved ~ victim_age + victim_race + victim_sex, data = .x, family = binomial())),
    results = map(models, broom::tidy)) %>% 
  select(-data, -models) %>% 
  unnest(results) %>% 
  mutate(lower_CI = exp(estimate - 1.96*std.error),
         upper_CI = exp(estimate + 1.96*std.error),
         OR = exp(estimate)) %>%
  select(city_state, term, OR, lower_CI, upper_CI) %>%
  filter(term == "victim_sexMale")

fit_all %>%
  knitr::kable(digits = 3)

```

Now we will make a plot showing the OR and 95% CI for each city.

```{r}

fit_all %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) +        
    geom_point() +
    geom_errorbar(aes(ymin = lower_CI, ymax = upper_CI)) +
    theme(axis.text.x = element_text(angle = 80, hjust = 1))

```

From this plot, we can see the city with the lowest OR for solved homicides comparing male victims to females while adjusting for race and age is New York at 0.26 and the highest is Albuquerque at 1.77.

The interpretations are as follows: 

In New York City, the odds of solving a homicide case for a male is 0.26 times the odds of solving a homicide case for a female, adjusting for race and age. In Albuquerque, the odds of solving a homicide case for a male is 1.77 times the odds of solving a homicide case for a female, adjusting for race and age.

However, the confidence interval for Albuquerque includes the null value of 1 making the OR not statistically significant whereas 1 is not in the confidence interval for New York, which means the OR is statistically significant. We would need to double check with the p-values.


## Problem 3


We will start by loading and cleaning the data in the following way: 


```{r}
children = read.csv("./birthweight.csv")

children = 
  children %>%
  janitor::clean_names() %>%
  mutate(id = 1:4342,
         babysex = recode(babysex,
                          "1" = "male",
                          "2" = "female"),
         babysex = factor(babysex, levels = c("male", "female")))

```


Now we will construct a regression model that predicts birthweight using the given predictors in the dataset. I choose to use the variables `gaweeks`, `ppbmi`, and `smoken`. I choose these variables because there has been research and evidence that being overweight or obese, smoking, and gestational age at birth can all have negative impacts on a baby's birthweight. When constructing plots to visually examine a relationship between these variables and birthweight, there is not obvious relationship that can be deciphered as seen below. However, for the `gaweeks` variable, we can see somewhat of a linear relationship.


```{r}

children %>%
  ggplot(aes(x = ppwt, y = bwt)) +
  geom_point()

children %>%
  ggplot(aes(x = smoken, y = bwt)) +
  geom_point()

children %>%
  ggplot(aes(x = gaweeks, y = bwt)) +
  geom_point()

```


Below is the model I have constructed and a plot of the residuals vs. the fitted values. I have also plotted the true `bwt` values in red. When you calculate the `rmse` for the model, the value is 456.66, which is very high and tells us that for this model, the residuals are very spread out and not very concentrated around the model regression line. 


```{r}
linear_mod = lm(bwt ~ gaweeks + ppwt + smoken, data = children)

children %>%
  add_predictions(linear_mod) %>%
  add_residuals(linear_mod, var = "resid") %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  geom_point(aes(y = bwt), color = "red")


rmse(linear_mod, children)


```


Now we will construct and compare my model to the 2 other models of interest for this problem. The `rmse` values are still high but much lower compared to my proposed model which shows an early indication that they may better predict `bwt`.


```{r}

mod_two = lm(bwt ~ blength + gaweeks, data = children)

mod_three = lm(bwt ~ babysex * blength * bhead, data = children)

rmse(linear_mod, children)
rmse(mod_two, children)
rmse(mod_three, children)


```


We will finish the comparison by using cross-validation as below:


```{r}

cv_df = 
  crossv_mc(children, 100) %>%
  mutate(
    train = map(train, as_tibble),
    test =  map(test, as_tibble)
  ) %>%
  mutate(
    linear_fit =    map(.x = train, ~lm(bwt ~ gaweeks + ppbmi + smoken,  data = .x)),
    mod_two_fit =   map(.x = train, ~lm(bwt ~ blength + gaweeks,         data = .x)),
    mod_three_fit = map(.x = train, ~lm(bwt ~ babysex * blength * bhead, data = .x))
  ) %>%
  mutate(
    rmse_linear =    map2_dbl(.x = linear_fit,    .y = test, ~rmse(model = .x, data = .y)),
    rmse_mod_two =   map2_dbl(.x = mod_two_fit,   .y = test, ~rmse(model = .x, data = .y)),
    rmse_mod_three = map2_dbl(.x = mod_three_fit, .y = test, ~rmse(model = .x, data = .y)))


cv_df %>% 
  select(starts_with("rmse")) %>%
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "RMSE",
    names_prefix = "rmse_"
  ) %>% 
  ggplot(aes(x = model, y = RMSE)) +
    geom_boxplot()
```


According to the boxplots generated, we can see that the model with the highest `rmse` value is my model and the one with the lowest is model three with the interaction terms. This means that model three predicts `bwt` best with its model line having the least amount of residual deviation from the line of best fit. The best model to use would be model 3.

